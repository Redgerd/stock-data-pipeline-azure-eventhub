{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d04ad46-f0e5-436a-9f2e-d39eece4cb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dlt\n",
    "%pip install confluent_kafka\n",
    "%pip install requests\n",
    "%pip install confluent_kafka\n",
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13cf7fab-a921-4253-90b4-e14be37829cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError, TopicPartition\n",
    "import json  # JSON module to deserialize the data\n",
    "import pandas as pd  # Pandas for DataFrame\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, ArrayType\n",
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "564f1038-5ffc-4114-adec-aa0b69e05428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define inner schema for results array\n",
    "inner_schema = StructType([\n",
    "    StructField(\"v\", DoubleType(), True),   # volume traded\n",
    "    StructField(\"vw\", DoubleType(), True),  # volume weighted avg price (not used)\n",
    "    StructField(\"o\", DoubleType(), True),   # open price\n",
    "    StructField(\"c\", DoubleType(), True),   # close price\n",
    "    StructField(\"h\", DoubleType(), True),   # high price\n",
    "    StructField(\"l\", DoubleType(), True),   # low price\n",
    "    StructField(\"t\", LongType(), True),     # trade date (Unix timestamp)\n",
    "    StructField(\"n\", LongType(), True)      # number of transactions (not used)\n",
    "])\n",
    "\n",
    "# Outer schema\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"request_id\", StringType(), True),\n",
    "    StructField(\"results\", ArrayType(inner_schema), True)\n",
    "])\n",
    "\n",
    "# ------------------- Bronze Layer -------------------\n",
    "@dlt.table(\n",
    "    comment=\"Bronze layer - raw stock data from Azure Event Hub via Kafka\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_stock_data_test_Sultaan():\n",
    "    kafka_bootstrap_servers = \"\"\n",
    "    topic_name = \"\"\n",
    "    connection_string = \"\"\n",
    "\n",
    "    kafka_df = (\n",
    "        spark.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "        .option(\"subscribe\", topic_name)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "        .option(\"kafka.sasl.jaas.config\", \n",
    "                f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";')\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    # Parse JSON\n",
    "    kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp as kafka_timestamp\")\n",
    "\n",
    "    json_df = kafka_df.select(from_json(col(\"json_value\"), schema).alias(\"data\"), \"kafka_timestamp\") \\\n",
    "        .selectExpr(\n",
    "            \"data.ticker as stock_symbol\",\n",
    "            \"data.request_id as request_id\",\n",
    "            \"data.results[0].t as trade_date\",\n",
    "            \"data.results[0].o as open_price\",\n",
    "            \"data.results[0].h as high_price\",\n",
    "            \"data.results[0].l as low_price\",\n",
    "            \"data.results[0].c as close_price\",\n",
    "            \"data.results[0].v as volume_traded\",\n",
    "            \"kafka_timestamp\"\n",
    "        )\n",
    "\n",
    "    return json_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff86e2d7-5356-4c59-8762-7fd33d07e0ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Layer\n",
    "@dlt.table(\n",
    "    comment=\"Silver layer - cleansed stock data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_stock_data_testing_Sultaan():\n",
    "    return (\n",
    "        dlt.read(\"bronze_stock_data_test_Sultaan\")\n",
    "        .filter(\"stock_symbol IS NOT NULL AND trade_date IS NOT NULL\")\n",
    "        .filter(\"open_price IS NOT NULL AND high_price IS NOT NULL AND low_price IS NOT NULL AND close_price IS NOT NULL\")\n",
    "        .filter(\"volume_traded > 0\")\n",
    "        .filter(\"kafka_timestamp IS NOT NULL\")\n",
    "        .select(\n",
    "            \"stock_symbol\",\n",
    "            \"request_id\",\n",
    "            \"trade_date\",\n",
    "            \"open_price\",\n",
    "            \"high_price\",\n",
    "            \"low_price\",\n",
    "            \"close_price\",\n",
    "            \"volume_traded\",\n",
    "            \"kafka_timestamp\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50ac188b-076a-4e66-9af5-63e6016f8f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold Layer\n",
    "@dlt.table(\n",
    "    comment=\"Gold layer - aggregated stock data for reporting\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_stock_data_testing_Sultaan():\n",
    "    return (\n",
    "        dlt.read(\"silver_stock_data_testing_Sultaan\")\n",
    "        .groupBy(\"stock_symbol\")\n",
    "        .agg(\n",
    "            F.sum(\"volume_traded\").alias(\"total_volume_traded\"),\n",
    "            F.avg(\"close_price\").alias(\"avg_close_price\"),\n",
    "            F.max(\"high_price\").alias(\"max_high_price\"),\n",
    "            F.min(\"low_price\").alias(\"min_low_price\"),\n",
    "            F.max(\"request_id\").alias(\"latest_request_id\"),\n",
    "            F.max(\"kafka_timestamp\").alias(\"latest_timestamp\")\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
